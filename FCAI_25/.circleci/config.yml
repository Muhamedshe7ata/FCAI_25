version: 2.1
commands:
  install_awscli:
    description: Install AWS CLI v2
    steps:
      - run:
          name: Install AWS CLI v2
          command: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install

  install_ansible:
    description: Install Ansible
    steps:
      - run:
          name: Install Ansible 
          command: |
            sudo apt update
            sudo apt install software-properties-common -y
            sudo add-apt-repository --yes --update ppa:ansible/ansible
            sudo apt install ansible -y

  install_nodejs:
    description: Install Node.js 13
    steps:
    - run:
        name: Install Node.js 13
        command: |
          curl -fsSL https://deb.nodesource.com/setup_13.x | sudo -E bash -
          sudo apt install -y nodejs
   

  destroy-environment:
    description: Destroy back-end and front-end cloudformation stacks given a workflow ID.
    parameters:
      Workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}
    steps:
      - run:
          name: Destroy environments
          # Consider if 'when: on_fail' is truly what you want for the destroy logic
          # If you always want to run this destroy step when the command is called, remove 'when: on_fail'
          when: on_fail
          command: |
            echo "Attempting to destroy environment: << parameters.Workflow_ID >>" # Added for clarity
            aws cloudformation delete-stack --stack-name udapeople-backend-<< parameters.Workflow_ID >> || echo "Failed to delete backend stack or stack does not exist."
            aws s3 rm s3://udapeople-<<parameters.Workflow_ID>> --recursive || echo "Failed to remove S3 bucket or bucket does not exist."
            aws cloudformation delete-stack --stack-name udapeople-frontend-<< parameters.Workflow_ID >> || echo "Failed to delete frontend stack or stack does not exist."
      # --- REMOVE THE LINE BELOW ---
      # - destroy-environment

      # --- Consider if this persist step really belongs in the destroy command ---
      # Usually, you persist data needed by *later* jobs. Destroy is often the *last* thing.
      - persist_to_workspace:
          root: ~/project
          paths:
           - project/.circleci/ansible/inventory.txt

  revert-migrations:
    description: Revert database migrations
    parameters:
      Workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}
        description: "The workflow ID (or relevant part) to check migration status against."
    steps:
      - run:
          name: Revert migrations
          when: on_fail
          command: |
            SUCCESS=$(curl --insecure https://kvdb.io/${KVDB_BUCKET}/migration_ << parameters.Workflow_ID >>)
            # Logic for reverting the database state
            if [[ $SUCCESS -eq 1 ]]; then
              cd ~/project/backend
              npm install
              npm run migration:revert
            fi

            
jobs:
  build-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Build front-end
          command: |
             cd frontend
              npm install
              npm run build

      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-deps
      # - notify_on_failure
  build-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end build
          command: |
            cd backend
            npm install
            npm run build
      - save_cache:
          paths: [backend/node_modules]
          key: backend-deps
      # - notify_on_failure

  test-frontend:
    docker:

      - image: cimg/node:13.8.0
    steps:
      - checkout 
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Front-end Unit Test
          command: |
            cd frontend
            npm install
            npm test
      # - notify_on_failure
   
                
  test-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end  Unit Test
          command: |
            cd backend
            npm install
            npm test
      # - notify_on_failure


            
  scan-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Front-end  scan
          command: |
            cd frontend
            npm install
            npm audit --audit-level=critical || true
      # - notify_on_failure
  scan-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end security scan
          command: |
            cd backend
            npm install
            npm audit fix --force --audit-level=critical
            npm audit fix --force --audit-level=critical
            npm audit --audit-level=critical
     
  deploy-infrastructure:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - run:
          name: Install AWS CLI
          command: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install
      - run:
          name: Ensure back-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/backend.yml \
              --tags project=udapeople\
              --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
              
      - run:
          name: Ensure front-end infrastructure exist
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=your-tag \
              --stack-name "udapeople-frontend-$(echo "${CIRCLE_WORKFLOW_ID:0:7}" | tr '[:upper:]' '[:lower:]')" \
              --parameter-overrides ID=$(echo "${CIRCLE_WORKFLOW_ID:0:7}" | tr '[:upper:]' '[:lower:]')  
              
      # - run:
      #     name: Add back-end ip to ansible inventory
      #     command: |  
      #       BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
      #         --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
      #         --query 'Reservations[*].Instances[*].PublicIpAddress' \
      #         --output text)
      #       echo $BACKEND_PUBLIC_IP >> .circleci/ansible/inventory.txt
      #       cat .circleci/ansible/inventory.txt
      # - persist_to_workspace:
      #     root: ~/
      #     paths:
      #       - project/.circleci/ansible/inventory.txt
      #  # - destroy-environment
      - run:
         name: Add back-end ip to ansible inventory
         command: |

                    echo "Attempting to fetch IP for tag: Name=backend-${CIRCLE_WORKFLOW_ID:0:7}"
       
                    BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
                      --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
                      --query 'Reservations[*].Instances[*].PublicIpAddress' \
                      --output text)

                    echo "IP Address Fetched: [$BACKEND_PUBLIC_IP]" # Debug echo

                    # Create/Overwrite the inventory file with the group header
                    # Make sure the path here is correct relative to the project root
                    echo "[web]" > .circleci/ansible/inventory.txt

                    # Append the fetched IP address to the inventory file
                    echo "$BACKEND_PUBLIC_IP" >> .circleci/ansible/inventory.txt

                    echo "Final contents of inventory file in this step:"
                    # Make sure the path here is correct relative to the project root
                    cat .circleci/ansible/inventory.txt        
                              
      - run:
          name: Debug - Check Inventory Before Persist
          command: |
            echo "--- Content of inventory.txt before persisting ---"
            cat ~/project/.circleci/ansible/inventory.txt || echo "File not found before persisting!"
            echo "--------------------------------------------------"
            echo "--- Files in .circleci/ansible before persisting ---"
            ls -la ~/project/.circleci/ansible/ || echo "Directory not found before persisting!"
            echo "----------------------------------------------------"           
      - persist_to_workspace:
          root: ~/project
          paths:
            - .circleci/ansible/inventory.txt # This path seems correct assuming your working_directory is ~/project
      ###################
  # configure-infrastructure:
  #   docker:
  #     - image: cimg/base:stable # Or similar base image
  #   steps:
  #     - checkout
  #     - install_ansible # This seems to be a custom or orb step
  #     - add_ssh_keys:
  #         fingerprints: ["SHA256:FJogcncZoAfN0hgvqhqDgAiiOafEii9G+HsAStU+Ok4"] # Placeholder for your actual fingerprint
  #     - attach_workspace:
  #         at: ~/
  #     - run:
  #         name: Configure Server
  #         command: |
  #           cd .circleci/ansible
  #           cat inventory.txt # This line was added to show the inventory content
  #           ansible-playbook -i inventory.txt configure-server.yml
  configure-infrastructure:
    docker:
      - image: cimg/base:stable # Use a suitable base image with basic tools
    steps:
      - checkout # Checkout your code repository, including .circleci/ansible
      - install_awscli # You need AWS CLI to fetch the dynamic IP
      # Assume install_ansible and add_ssh_keys are custom/orb steps that work
      - install_ansible
      - add_ssh_keys:
          fingerprints: ["SHA256:FJogcncZoAfN0hgvqhqDgAiiOafEii9G+HsAStU+Ok4"] # Use your actual fingerprint

      # --- STEP 1: GENERATE DYNAMIC INVENTORY ---
      - run:
          name: Generate Dynamic Ansible Inventory
          command: |
            echo "--- Starting Dynamic Inventory Generation Step ---"
            pwd # Print current directory (should be ~/project if working_directory is not set)

            # Make sure your AWS Region is configured! Either via CircleCI env var AWS_DEFAULT_REGION
            # or explicitly set it here if it's consistent
            export AWS_REGION=${AWS_DEFAULT_REGION:-eu-north-1} # Example, change region if needed

            echo "--- Fetching EC2 Instance IP based on tag 'Name=backend-b4ed3be' (as seen in screenshot logs) ---"
            # !!! ADJUST THIS aws command to find YOUR specific backend EC2 instance reliably !!!
            # This filter example is taken from your log output (Name=backend-b4ed3be). Update if needed.
            # Add additional filters like instance-state-name=running if necessary.
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
              --filters "Name=instance-state-name,Values=running" "Name=tag:Name,Values=backend-b4ed3be" \
              --query "Reservations[*].Instances[*].PublicIpAddress" --output text --region ${AWS_REGION})

            echo "AWS CLI Command Output for IP: '${BACKEND_PUBLIC_IP}'"

            # Handle case where IP is not found (very important!)
            if [ -z "$BACKEND_PUBLIC_IP" ]; then
              echo "ERROR: Could NOT fetch backend public IP via AWS CLI. Check filters, region, and instance state."
              exit 1 # Fail the job as we can't proceed without the IP
            fi

            echo "Successfully Fetched IP: ${BACKEND_PUBLIC_IP}" # Confirm IP is in variable

            # Define the path to your inventory file relative to the project root
            INVENTORY_DIR=".circleci/ansible"
            INVENTORY_FILE="${INVENTORY_DIR}/inventory.txt"

            # Ensure the ansible directory exists before writing
            mkdir -p ${INVENTORY_DIR}

            echo "--- Writing dynamic inventory file to ${INVENTORY_FILE} ---"
            # Overwrite the file with the correct format
            echo "[webserver]" > ${INVENTORY_FILE}
            echo "${BACKEND_PUBLIC_IP}" >> ${INVENTORY_FILE} # Write the actual fetched IP
            echo "[all:children]" >> ${INVENTORY_FILE}
            echo "webserver" >> ${INVENTORY_FILE} # Make 'webserver' part of the 'all' group

            echo "--- Final content of dynamically generated inventory file: ---"
            cat ${INVENTORY_FILE} || echo "ERROR: Could NOT read inventory file after writing!" # Verify content
            echo "-------------------------------------------------------------"
            echo "--- Files in .circleci/ansible directory after generation ---"
            ls -la ${INVENTORY_DIR} || echo "Directory not found or empty!" # Verify file is there
            echo "-------------------------------------------------------------"

      # --- STEP 2: RUN CONFIGURE-SERVER PLAYBOOK (NOW WITH POPULATED INVENTORY) ---
      - run:
          name: Run Configure Server Playbook
          command: |
            echo "--- Entering Run Configure Server Playbook Step ---"
            pwd # Show current directory (should be ~/project)

            # Re-verify inventory content *just before* running playbook
            echo "--- Content of inventory.txt just before running configure-server.yml ---"
            cat .circleci/ansible/inventory.txt || echo "ERROR: inventory.txt is missing or unreadable just before playbook!"
            echo "--------------------------------------------------------------------"

            # Change directory to where playbook and the populated inventory are located
            cd .circleci/ansible

            # Run the playbook using the populated inventory file
            # Added -v for slightly more verbose Ansible output to help debugging if it still skips
            ansible-playbook -i inventory.txt configure-server.yml -v

            echo "--- Finished Run Configure Server Playbook ---"

      # --- STEP 3: PERSIST POPULATED INVENTORY AND ANSIBLE DIRECTORY ---
      # Save the entire .circleci/ansible directory to the workspace, including the generated inventory.txt
      # This makes it available for the deploy-backend job.
      - persist_to_workspace:
          root: ~/project # Standard location after checkout
          paths:
            - .circleci/ansible # Save the entire directory containing playbook and populated inventory
            
  run-migrations:
    docker:
      - image:  cimg/node:13.8.0
    steps:
      - checkout
      - install_awscli
      - restore_cache:
          keys: [backend-build]
      - run:
          name: prepare environment for backend build
          command: |
            echo "ENVIRONMENT=production" > backend/.env
            echo "VERSION=1" >> backend/.env
            echo "TYPEORM_CONNECTION=postgres" >> backend/.env
            echo "TYPEORM_MIGRATIONS_DIR=./src/migrations" >> backend/.env
            echo "TYPEORM_ENTITIES=./src/modules/domain/**/*.entity.ts" >> backend/.env
            echo "TYPEORM_MIGRATIONS=./src/migrations/*.ts" >> backend/.env
            echo "TYPEORM_HOST=${TYPEORM_HOST}" >> backend/.env
            echo "TYPEORM_PORT=${TYPEORM_PORT}" >> backend/.env
            echo "TYPEORM_USERNAME=${TYPEORM_USERNAME}" >> backend/.env
            echo "TYPEORM_PASSWORD=${TYPEORM_PASSWORD}" >> backend/.env
            echo "TYPEORM_DATABASE=${TYPEORM_DATABASE}" >> backend/.env
            echo "DB_SSL=${DB_SSL}" >> backend/.env
            echo "NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}" >> backend/.env
            echo "PGSSLMODE=require" >> backend/.env
      - run:
          name: Install dependencies
          command: |
            export DEBIAN_FRONTEND=noninteractive
          
            sudo apt update
            sudo apt install -y awscli curl
            
      - run:
          name: Run migrations
          command: |
            cd backend
            npm ci
            if [ -f .env ]; then
              export $(grep -v '^#' .env | xargs)
            fi
            if [ -z "${KVDB_BUCKET}" ]; then
              echo "ERROR: KVDB_BUCKET environment variable is not set. Cannot update KVDB."
              exit 1
            fi
            npm run migrations > ./migrations_dump.txt 2>&1
            migration_status_code=$?
            cat ./migrations_dump.txt
            if [[ "${migration_status_code}" -ne 0 ]]; then
              echo "ERROR: 'npm run migrations' command failed with exit code ${migration_status_code}."
              exit "${migration_status_code}"
            fi
            echo "Migration command completed successfully (exit code 0). Checking output for success messages..."
            if grep -E -q "has been executed successfully.|No migrations are pending" ./migrations_dump.txt; then
              echo "Found expected success message ('migration executed' or 'no migrations pending') in log."
              echo "--- Updating KVDB status key ---"
              curl -sf -X PUT \
                "https://kvdb.io/${KVDB_BUCKET}/migration_${CIRCLE_WORKFLOW_ID:0:7}" \
                -d '1' || {
                  curl_exit_code=$?
                  echo "ERROR: curl command failed to update kvdb.io with exit code ${curl_exit_code}."
                  echo "Check network connectivity, KVDB_BUCKET value ('${KVDB_BUCKET}'), and kvdb.io service status."
                  exit $curl_exit_code
                }
              echo "KVDB status updated successfully via curl."
            else
              echo "WARNING: Migration command succeeded (exit code 0), but NEITHER 'has been executed successfully.' NOR 'No migrations are pending' was found in the output."
              echo "Skipping KVDB update due to unrecognized success output."
            fi
            echo "Migration and KVDB update check process finished successfully."
      - destroy-environment
      - revert-migrations


             
        

  deploy-frontend:
    docker: 
    - image: cimg/base:stable
    steps:
      - checkout
      - install_awscli
      - install_nodejs
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Install dependencies
          command: |
            cd frontend
            npm install
      - run:
          name: Get backend url
          command: |
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --query 'Reservations[*].Instances[*].PublicIpAddress' \
              --output text)

            echo "API_URL=http://${BACKEND_PUBLIC_IP}:3030" >> frontend/.env
            cat frontend/.env
      - run:
          name: Deploy frontend objects
          command: |
            cd frontend
            npm run build
            aws s3 cp dist s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7} --recursive
  deploy-backend:
    docker:
      - image: cimg/base:stable # Use a suitable base image
    steps:
      # --- STEP 1: ATTACH WORKSPACE FROM configure-infrastructure JOB ---
      - attach_workspace:
          at: ~/project # This must exactly match the 'root' in persist_to_workspace above
      # You might need checkout here if your source code (backend, ansible playbook files, etc.)
      # is NOT included in the workspace persistence from the previous job.
      - checkout # Checkout source code (important if not fully in workspace)

      # Install dependencies needed for THIS job (node, npm, etc.)
      # Only install if not covered by the attached workspace or base image
      - install_nodejs # Example, ensure you have necessary steps to run npm commands
      # install_ansible should already be present if attached or if it's in the image

      # --- STEP 2: VERIFY ATTACHED INVENTORY (DEBUG) ---
      - run:
          name: Verify Attached Inventory in Deploy Job
          command: |
            echo "--- Verifying Inventory After Workspace Attachment in deploy-backend Job ---"
            pwd # Show current directory (should be ~/project)
            
            # Check for the expected directory structure and file
            echo "--- Files in expected ansible directory after attachment ---"
            ls -la ~/project/.circleci/ansible/ || echo "ERROR: Directory .circleci/ansible not found after attachment!"
            echo "----------------------------------------------------------"
            
            # Check the content of the attached inventory file
            echo "--- Content of inventory.txt after workspace attachment ---"
            cat ~/project/.circleci/ansible/inventory.txt || echo "ERROR: inventory.txt NOT FOUND or EMPTY AFTER ATTACHING WORKSPACE!"
            echo "--------------------------------------------------------"
            echo "--- End of Verification ---"

      # --- STEP 3: BUILD AND PACKAGE BACKEND (Keep your existing logic) ---
      - run:
          name: Build and Package Backend
          command: |
            # Navigate to backend code (path relative to where code was checked out/attached)
            cd ~/project/backend # ADJUST THIS PATH if backend code isn't here

            # Ensure node_modules are installed if needed for build/package
            # (These might need to be installed HERE if they aren't persisted or built previously)
            npm install 
            npm run build # Run your backend build command (should produce JS files, etc.)

            cd ~/project # Go back to the root

            # Package backend artifact (ensure 'backend' dir exists and is in the right place)
            tar -C backend -czvf artifact.tar.gz .
            
            # Copy artifact to the deploy role files directory within the ansible directory
            # This ansible directory was ATTACHED from the workspace
            mkdir -p .circleci/ansible/roles/deploy/files # Ensure target directory exists
            cp artifact.tar.gz .circleci/ansible/roles/deploy/files # Copy artifact here

      # --- STEP 4: RUN DEPLOY-BACKEND PLAYBOOK (Using the Attached Inventory) ---
      - run:
          name: Run Deploy Backend Playbook
          command: |
            echo "--- Entering Run Deploy Backend Playbook Step ---"
            pwd # Show current directory (should be ~/project)
            
            # Re-verify inventory content AGAIN just before running playbook
            echo "--- Content of inventory.txt just before running deploy-backend.yml ---"
            cat ~/project/.circleci/ansible/inventory.txt || echo "ERROR: inventory.txt is missing or unreadable just before playbook!"
            echo "--------------------------------------------------------------------"

            # Change directory to where playbook and attached inventory are located
            cd ~/project/.circleci/ansible

            # Run the playbook using the attached inventory file
            # Add -v for slightly more verbose output
            ansible-playbook -i inventory.txt deploy-backend.yml -v

            echo "--- Finished Run Deploy Backend Playbook ---"

      # Note: destroy-environment and revert-migrations steps were logically problematic after a successful deploy.
      # You might want to remove them unless they are intended for cleanup *after* the entire workflow finishes,
      # and are configured conditionally on workflow success/failure.
      # - destroy-environment:
      #     workflow_id: "${CIRCLE_WORKFLOW_ID:0:7}"
      # - revert-migrations:
      #     workflow_id: "${CIRCLE_WORKFLOW_ID:0:7}"
smoke-test:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            sudo apt update
            sudo apt upgrade -y
            sudo apt install -y curl
            sudo apt install -y awscli
            aws --version
      - run:
          name: Backend smoke test.
          command: |       
              #!/bin/bash -eo pipefail
              STACK_SUFFIX=${CIRCLE_WORKFLOW_ID:0:7}
              BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
               --filters "Name=tag:aws:cloudformation:stack-name,Values=udapeople-backend-${STACK_SUFFIX}" "Name=instance-state-name,Values=running" \
              --query 'Reservations[].Instances[].PublicIpAddress' \
              --output text)

              # Check if BACKEND_PUBLIC_IP is empty
              if [ -z "$BACKEND_PUBLIC_IP" ]; then
                echo "Error: No EC2 instances found with the specified tag."
                exit 1
              fi

              echo "Retrieved BACKEND_PUBLIC_IP: $BACKEND_PUBLIC_IP"

              # Set API URL
              export API_URL=http://${BACKEND_PUBLIC_IP}:3030

              echo "API URL: $API_URL"

              # Check API status
              RESPONSE=$(curl -s $API_URL/api/status)
              echo "API Response: $RESPONSE"

              if echo "$RESPONSE" | grep "ok"; then
                echo "Smoke test passed."
                exit 0
              else 
                echo "Smoke test failed. 'ok' not found in API response."
                exit 1 
              fi 

                  

      - run:
          name: Frontend smoke test.
          command: |
            URL="http://udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website.${AWS_DEFAULT_REGION}.amazonaws.com/#/employees"
            echo ${URL}
            if curl -s ${URL} | grep "Welcome"
            then
              echo SUCCESS
              exit 0
            else
              echo FAIL
              exit 1
            fi
 

workflows:
  default:
    jobs:
      - build-frontend
      - build-backend
      - test-frontend:
          requires: [build-frontend]
      - test-backend:
          requires: [build-backend]
      - scan-backend:
          requires: [build-backend]
      - scan-frontend:
          requires: [build-frontend]
 
      - deploy-infrastructure:
          requires: [test-frontend, test-backend, scan-frontend, scan-backend]
          filters:
            branches:
              only: [master]
      - configure-infrastructure:
          requires: [deploy-infrastructure]
      - run-migrations:
          requires: [configure-infrastructure]
      - deploy-frontend:
          requires: [run-migrations]
      - deploy-backend:
          requires: [configure-infrastructure]
      - smoke-test:
          requires: [deploy-backend, deploy-frontend]
 

#####################################################


