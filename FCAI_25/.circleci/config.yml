version: 2.1
commands:
  install_awscli:
    description: Install AWS CLI v2
    steps:
      - run:
          name: Install AWS CLI v2
          command: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install

  install_ansible:
    description: Install Ansible
    steps:
      - run:
          name: Install Ansible 
          command: |
            sudo apt update
            sudo apt install software-properties-common -y
            sudo add-apt-repository --yes --update ppa:ansible/ansible
            sudo apt install ansible -y

  install_nodejs:
    description: Install Node.js 13
    steps:
    - run:
        name: Install Node.js 13
        command: |
          curl -fsSL https://deb.nodesource.com/setup_13.x | sudo -E bash -
          sudo apt install -y nodejs
   

  destroy-environment:
    description: Destroy back-end and front-end cloudformation stacks given a workflow ID.
    parameters:
      Workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}
    steps:
      - run:
          name: Destroy environments
          # Consider if 'when: on_fail' is truly what you want for the destroy logic
          # If you always want to run this destroy step when the command is called, remove 'when: on_fail'
          when: on_fail
          command: |
            echo "Attempting to destroy environment: << parameters.Workflow_ID >>" # Added for clarity
            aws cloudformation delete-stack --stack-name udapeople-backend-<< parameters.Workflow_ID >> || echo "Failed to delete backend stack or stack does not exist."
            aws s3 rm s3://udapeople-<<parameters.Workflow_ID>> --recursive || echo "Failed to remove S3 bucket or bucket does not exist."
            aws cloudformation delete-stack --stack-name udapeople-frontend-<< parameters.Workflow_ID >> || echo "Failed to delete frontend stack or stack does not exist."
      # --- REMOVE THE LINE BELOW ---
      # - destroy-environment

      # --- Consider if this persist step really belongs in the destroy command ---
      # Usually, you persist data needed by *later* jobs. Destroy is often the *last* thing.
      - persist_to_workspace:
          root: ~/project
          paths:
           - project/.circleci/ansible/inventory.txt

  revert-migrations:
    description: Revert database migrations
    parameters:
      Workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}
        description: "The workflow ID (or relevant part) to check migration status against."
    steps:
      - run:
          name: Revert migrations
          when: on_fail
          command: |
            SUCCESS=$(curl --insecure https://kvdb.io/${KVDB_BUCKET}/migration_ << parameters.Workflow_ID >>)
            # Logic for reverting the database state
            if [[ $SUCCESS -eq 1 ]]; then
              cd ~/project/backend
              npm install
              npm run migration:revert
            fi

            
jobs:
  build-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Build front-end
          command: |
             cd frontend
              npm install
              npm run build

      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-deps
      # - notify_on_failure
  build-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end build
          command: |
            cd backend
            npm install
            npm run build
      - save_cache:
          paths: [backend/node_modules]
          key: backend-deps
      # - notify_on_failure

  test-frontend:
    docker:

      - image: cimg/node:13.8.0
    steps:
      - checkout 
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Front-end Unit Test
          command: |
            cd frontend
            npm install
            npm test
      # - notify_on_failure
   
                
  test-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end  Unit Test
          command: |
            cd backend
            npm install
            npm test
      # - notify_on_failure


            
  scan-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Front-end  scan
          command: |
            cd frontend
            npm install
            npm audit --audit-level=critical || true
      # - notify_on_failure
  scan-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end security scan
          command: |
            cd backend
            npm install
            npm audit fix --force --audit-level=critical
            npm audit fix --force --audit-level=critical
            npm audit --audit-level=critical
     
  deploy-infrastructure:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - run:
          name: Install AWS CLI
          command: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install
      - run:
          name: Ensure back-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/backend.yml \
              --tags project=udapeople\
              --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
              
      - run:
          name: Ensure front-end infrastructure exist
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=your-tag \
              --stack-name "udapeople-frontend-$(echo "${CIRCLE_WORKFLOW_ID:0:7}" | tr '[:upper:]' '[:lower:]')" \
              --parameter-overrides ID=$(echo "${CIRCLE_WORKFLOW_ID:0:7}" | tr '[:upper:]' '[:lower:]')  
              
      # - run:
      #     name: Add back-end ip to ansible inventory
      #     command: |  
      #       BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
      #         --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
      #         --query 'Reservations[*].Instances[*].PublicIpAddress' \
      #         --output text)
      #       echo $BACKEND_PUBLIC_IP >> .circleci/ansible/inventory.txt
      #       cat .circleci/ansible/inventory.txt
      # - persist_to_workspace:
      #     root: ~/
      #     paths:
      #       - project/.circleci/ansible/inventory.txt
      #  # - destroy-environment
      - run:
         name: Add back-end ip to ansible inventory
         command: |

                    echo "Attempting to fetch IP for tag: Name=backend-${CIRCLE_WORKFLOW_ID:0:7}"
       
                    BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
                      --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
                      --query 'Reservations[*].Instances[*].PublicIpAddress' \
                      --output text)

                    echo "IP Address Fetched: [$BACKEND_PUBLIC_IP]" # Debug echo

                    # Create/Overwrite the inventory file with the group header
                    # Make sure the path here is correct relative to the project root
                    echo "[web]" > .circleci/ansible/inventory.txt

                    # Append the fetched IP address to the inventory file
                    echo "$BACKEND_PUBLIC_IP" >> .circleci/ansible/inventory.txt

                    echo "Final contents of inventory file in this step:"
                    # Make sure the path here is correct relative to the project root
                    cat .circleci/ansible/inventory.txt        
                              
      - run:
          name: Debug - Check Inventory Before Persist
          command: |
            echo "--- Content of inventory.txt before persisting ---"
            cat ~/project/.circleci/ansible/inventory.txt || echo "File not found before persisting!"
            echo "--------------------------------------------------"
            echo "--- Files in .circleci/ansible before persisting ---"
            ls -la ~/project/.circleci/ansible/ || echo "Directory not found before persisting!"
            echo "----------------------------------------------------"           
      - persist_to_workspace:
          root: ~/project
          paths:
            - .circleci/ansible/inventory.txt # This path seems correct assuming your working_directory is ~/project
      ###################
  # configure-infrastructure:
  #   docker:
  #     - image: cimg/base:stable # Or similar base image
  #   steps:
  #     - checkout
  #     - install_ansible # This seems to be a custom or orb step
  #     - add_ssh_keys:
  #         fingerprints: ["SHA256:FJogcncZoAfN0hgvqhqDgAiiOafEii9G+HsAStU+Ok4"] # Placeholder for your actual fingerprint
  #     - attach_workspace:
  #         at: ~/
  #     - run:
  #         name: Configure Server
  #         command: |
  #           cd .circleci/ansible
  #           cat inventory.txt # This line was added to show the inventory content
  #           ansible-playbook -i inventory.txt configure-server.yml
  configure-infrastructure:
    docker:
      - image: cimg/base:stable # Or similar base image
    # ... docker and other initial steps like checkout, install_awscli, install_ansible ...
    steps:
      - checkout
      - install_awscli # NEED THIS HERE TO FETCH IP
      - install_ansible
      - add_ssh_keys:
          fingerprints: ["SHA256:FJogcncZoAfN0hgvqhqDgAiiOafEii9G+HsAStU+Ok4"]

      # --- NEW DEBUG/DYNAMIC INVENTORY RUN STEP ---
      - run:
          name: Debug Generate and Verify Inventory
          command: |
            echo "--- Starting Debug Generate Inventory Step ---"
            pwd # Show current directory (should be ~/project)

            # --- 1. Fetch Dynamic IP ---
            echo "--- Fetching EC2 Instance IP ---"
            # Make sure your AWS Region is set! (Replace 'eu-north-1' or set AWS_DEFAULT_REGION env var)
            export AWS_REGION="eu-north-1" # <<< Adjust region or remove if set as env var

            # !!! ADJUST FILTER to find YOUR specific backend EC2 instance !!!
            # Example: using a tag called 'InstanceRole' with value 'backend'
            # Replace with your actual method of finding the right instance
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
              --filters "Name=instance-state-name,Values=running" "Name=tag:InstanceRole,Values=backend" \
              --query "Reservations[*].Instances[*].PublicIpAddress" --output text --region ${AWS_REGION})

            echo "Fetched IP: ${BACKEND_PUBLIC_IP}"
            
            if [ -z "$BACKEND_PUBLIC_IP" ]; then
              echo "Error: AWS CLI failed to fetch the backend public IP."
              exit 1 # Fail job early if IP not found
            fi
            echo "Backend IP found: ${BACKEND_PUBLIC_IP}" # Confirm variable is populated


            # --- 2. Write to Inventory File ---
            # Make sure the directory exists where the inventory file should be
            mkdir -p .circleci/ansible
            INVENTORY_FILE_PATH=".circleci/ansible/inventory.txt" # Path relative to pwd (which is ~/project)

            echo "--- Writing to Inventory File: ${INVENTORY_FILE_PATH} ---"
            
            # Use single > to overwrite/create, double >> to append
            echo "[webserver]" > ${INVENTORY_FILE_PATH}
            echo "${BACKEND_PUBLIC_IP}" >> ${INVENTORY_FILE_PATH} # Write the IP here
            echo "[all:children]" >> ${INVENTORY_FILE_PATH}
            echo "webserver" >> ${INVENTORY_FILE_PATH}

            echo "--- Done writing to inventory file ---"


            # --- 3. Verify Written File Content BEFORE Ansible Run ---
            echo "--- Final Content of Generated Inventory File in configure-infrastructure (BEFORE running Ansible) ---"
            # Use 'cat' to display the file content we just wrote
            cat ${INVENTORY_FILE_PATH} || echo "ERROR: Could NOT read the inventory file just created!"
            echo "--------------------------------------------------------------------------------------------------"
            echo "--- Listing files in .circleci/ansible directory ---"
            ls -la .circleci/ansible/
            echo "---------------------------------------------------"

      # --- !!! Now, AFTER the above step, this 'Configure Server' step runs !!! ---
      # This run step executes the original commands, but inventory.txt *should* now exist and be populated.
      - run:
          name: Configure Server # Runs Ansible using the inventory created in the previous step
          command: |
            echo "--- Entering Configure Server Step ---"
            pwd # Should still be ~/project (or your working_directory)

            # Check inventory content AGAIN just to be super sure it's still there
            echo "--- Final Check: Content of inventory.txt just before Ansible-playbook ---"
            cat .circleci/ansible/inventory.txt || echo "ERROR: inventory.txt disappeared or is unreadable BEFORE PLAYBOOK!"
            echo "-----------------------------------------------------------------------"

            # Change directory to where playbook and inventory are
            cd .circleci/ansible
            
            # Run the playbook. It *should* find the populated inventory.txt now.
            # Add --verbose or -vvv for more debug info from Ansible itself
            ansible-playbook -i inventory.txt configure-server.yml --verbose

            echo "--- Finished Configure Server Step ---"

      # --- Crucially, persist the inventory file for the next job ---
      - persist_to_workspace:
          root: ~/project # Standard root
          paths:
            - .circleci/ansible 
            
  run-migrations:
    docker:
      - image:  cimg/node:13.8.0
    steps:
      - checkout
      - install_awscli
      - restore_cache:
          keys: [backend-build]
      - run:
          name: prepare environment for backend build
          command: |
            echo "ENVIRONMENT=production" > backend/.env
            echo "VERSION=1" >> backend/.env
            echo "TYPEORM_CONNECTION=postgres" >> backend/.env
            echo "TYPEORM_MIGRATIONS_DIR=./src/migrations" >> backend/.env
            echo "TYPEORM_ENTITIES=./src/modules/domain/**/*.entity.ts" >> backend/.env
            echo "TYPEORM_MIGRATIONS=./src/migrations/*.ts" >> backend/.env
            echo "TYPEORM_HOST=${TYPEORM_HOST}" >> backend/.env
            echo "TYPEORM_PORT=${TYPEORM_PORT}" >> backend/.env
            echo "TYPEORM_USERNAME=${TYPEORM_USERNAME}" >> backend/.env
            echo "TYPEORM_PASSWORD=${TYPEORM_PASSWORD}" >> backend/.env
            echo "TYPEORM_DATABASE=${TYPEORM_DATABASE}" >> backend/.env
            echo "DB_SSL=${DB_SSL}" >> backend/.env
            echo "NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}" >> backend/.env
            echo "PGSSLMODE=require" >> backend/.env
      - run:
          name: Install dependencies
          command: |
            export DEBIAN_FRONTEND=noninteractive
          
            sudo apt update
            sudo apt install -y awscli curl
            
      - run:
          name: Run migrations
          command: |
            cd backend
            npm ci
            if [ -f .env ]; then
              export $(grep -v '^#' .env | xargs)
            fi
            if [ -z "${KVDB_BUCKET}" ]; then
              echo "ERROR: KVDB_BUCKET environment variable is not set. Cannot update KVDB."
              exit 1
            fi
            npm run migrations > ./migrations_dump.txt 2>&1
            migration_status_code=$?
            cat ./migrations_dump.txt
            if [[ "${migration_status_code}" -ne 0 ]]; then
              echo "ERROR: 'npm run migrations' command failed with exit code ${migration_status_code}."
              exit "${migration_status_code}"
            fi
            echo "Migration command completed successfully (exit code 0). Checking output for success messages..."
            if grep -E -q "has been executed successfully.|No migrations are pending" ./migrations_dump.txt; then
              echo "Found expected success message ('migration executed' or 'no migrations pending') in log."
              echo "--- Updating KVDB status key ---"
              curl -sf -X PUT \
                "https://kvdb.io/${KVDB_BUCKET}/migration_${CIRCLE_WORKFLOW_ID:0:7}" \
                -d '1' || {
                  curl_exit_code=$?
                  echo "ERROR: curl command failed to update kvdb.io with exit code ${curl_exit_code}."
                  echo "Check network connectivity, KVDB_BUCKET value ('${KVDB_BUCKET}'), and kvdb.io service status."
                  exit $curl_exit_code
                }
              echo "KVDB status updated successfully via curl."
            else
              echo "WARNING: Migration command succeeded (exit code 0), but NEITHER 'has been executed successfully.' NOR 'No migrations are pending' was found in the output."
              echo "Skipping KVDB update due to unrecognized success output."
            fi
            echo "Migration and KVDB update check process finished successfully."
      - destroy-environment
      - revert-migrations


             
        

  deploy-frontend:
    docker: 
    - image: cimg/base:stable
    steps:
      - checkout
      - install_awscli
      - install_nodejs
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Install dependencies
          command: |
            cd frontend
            npm install
      - run:
          name: Get backend url
          command: |
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --query 'Reservations[*].Instances[*].PublicIpAddress' \
              --output text)

            echo "API_URL=http://${BACKEND_PUBLIC_IP}:3030" >> frontend/.env
            cat frontend/.env
      - run:
          name: Deploy frontend objects
          command: |
            cd frontend
            npm run build
            aws s3 cp dist s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7} --recursive
deploy-backend:
      docker:
        - image: cimg/base:stable
      steps:
        # Attach workspace here at the beginning
        - attach_workspace:
            at: ~/project # Must match root in persist_to_workspace from configure-infrastructure

        # Add checkout here if your code repo is NOT fully covered by the workspace
        - checkout 
        # Ensure dependencies like install_awscli, install_nodejs, install_ansible are
        # met in this job if needed for the deploy-backend steps
        
        # --- IMPORTANT ---
        # Remove the dynamic IP fetching and inventory generation logic
        # from the run: Deploy backend command in THIS job.
        # That should only happen once in configure-infrastructure.
        # THIS job receives the INVENTORY file via the workspace.
        # --- --- ---

        - run:
            name: Deploy backend # Now uses the attached inventory
            command: |
              # Debugging: Confirm the attached inventory file exists and has content in this job
              echo "--- Checking Inventory After Workspace Attachment in deploy-backend ---"
              cat ~/project/.circleci/ansible/inventory.txt || echo "ERROR: Inventory not found or empty AFTER ATTACHING WORKSPACE!"
              echo "------------------------------------------------------------------------"
              
              # ... (Keep the steps for building/packaging the backend here) ...
              cd ~/project/backend # Adjust path based on checkout/attachment
              npm install
              npm run build
              
              cd ~/project # Adjust path
              tar -C backend -czvf artifact.tar.gz .
              mkdir -p .circleci/ansible/roles/deploy/files
              cp artifact.tar.gz .circleci/ansible/roles/deploy/files
              # ... (End build/package) ...

              # Now navigate to where the PLAYBOOK AND the attached inventory are located
              cd ~/project/.circleci/ansible # This should now have the file attached from workspace

              # Run Ansible using the attached inventory file
              ansible-playbook -i inventory.txt deploy-backend.yml
              # ... rest of your deploy backend logic .
smoke-test:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            sudo apt update
            sudo apt upgrade -y
            sudo apt install -y curl
            sudo apt install -y awscli
            aws --version
      - run:
          name: Backend smoke test.
          command: |       
              #!/bin/bash -eo pipefail
              STACK_SUFFIX=${CIRCLE_WORKFLOW_ID:0:7}
              BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
               --filters "Name=tag:aws:cloudformation:stack-name,Values=udapeople-backend-${STACK_SUFFIX}" "Name=instance-state-name,Values=running" \
              --query 'Reservations[].Instances[].PublicIpAddress' \
              --output text)

              # Check if BACKEND_PUBLIC_IP is empty
              if [ -z "$BACKEND_PUBLIC_IP" ]; then
                echo "Error: No EC2 instances found with the specified tag."
                exit 1
              fi

              echo "Retrieved BACKEND_PUBLIC_IP: $BACKEND_PUBLIC_IP"

              # Set API URL
              export API_URL=http://${BACKEND_PUBLIC_IP}:3030

              echo "API URL: $API_URL"

              # Check API status
              RESPONSE=$(curl -s $API_URL/api/status)
              echo "API Response: $RESPONSE"

              if echo "$RESPONSE" | grep "ok"; then
                echo "Smoke test passed."
                exit 0
              else 
                echo "Smoke test failed. 'ok' not found in API response."
                exit 1 
              fi 

                  

      - run:
          name: Frontend smoke test.
          command: |
            URL="http://udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website.${AWS_DEFAULT_REGION}.amazonaws.com/#/employees"
            echo ${URL}
            if curl -s ${URL} | grep "Welcome"
            then
              echo SUCCESS
              exit 0
            else
              echo FAIL
              exit 1
            fi
 

workflows:
  default:
    jobs:
      - build-frontend
      - build-backend
      - test-frontend:
          requires: [build-frontend]
      - test-backend:
          requires: [build-backend]
      - scan-backend:
          requires: [build-backend]
      - scan-frontend:
          requires: [build-frontend]
 
      - deploy-infrastructure:
          requires: [test-frontend, test-backend, scan-frontend, scan-backend]
          filters:
            branches:
              only: [master]
      - configure-infrastructure:
          requires: [deploy-infrastructure]
      - run-migrations:
          requires: [configure-infrastructure]
      - deploy-frontend:
          requires: [run-migrations]
      - deploy-backend:
          requires: [deploy-infrastructure]
      - smoke-test:
          requires: [deploy-backend, deploy-frontend]
 

#####################################################


