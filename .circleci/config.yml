version: 2.1
commands:
  install_awscli:
    description: Install AWS CLI v2
    steps:
      - run:
          name: Install AWS CLI v2
          command: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install

  install_ansible:
    description: Install Ansible
    steps:
      - run:
          name: Install Ansible 
          command: |
            sudo apt update
            sudo apt install software-properties-common -y
            sudo add-apt-repository --yes --update ppa:ansible/ansible
            sudo apt install ansible -y

  install_nodejs:
    description: Install Node.js 13
    steps:
    - run:
        name: Install Node.js 13
        command: |
          curl -fsSL https://deb.nodesource.com/setup_13.x | sudo -E bash -
          sudo apt install -y nodejs
   

  destroy-environment:
    description: Destroy back-end and front-end cloudformation stacks given a workflow ID.
    parameters:
      Workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}
    steps:
      - run:
          name: Destroy environments
          # Consider if 'when: on_fail' is truly what you want for the destroy logic
          # If you always want to run this destroy step when the command is called, remove 'when: on_fail'
          when: on_fail
          command: |
            echo "Attempting to destroy environment: << parameters.Workflow_ID >>" # Added for clarity
            aws cloudformation delete-stack --stack-name udapeople-backend-<< parameters.Workflow_ID >> || echo "Failed to delete backend stack or stack does not exist."
            aws s3 rm s3://udapeople-<<parameters.Workflow_ID>> --recursive || echo "Failed to remove S3 bucket or bucket does not exist."
            aws cloudformation delete-stack --stack-name udapeople-frontend-<< parameters.Workflow_ID >> || echo "Failed to delete frontend stack or stack does not exist."
      # --- REMOVE THE LINE BELOW ---
      # - destroy-environment

      # --- Consider if this persist step really belongs in the destroy command ---
      # Usually, you persist data needed by *later* jobs. Destroy is often the *last* thing.
      - persist_to_workspace:
          root: ~/
          paths:
           - project/.circleci/ansible/inventory.txt

  revert-migrations:
    description: Revert database migrations
    parameters:
      Workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}
        description: "The workflow ID (or relevant part) to check migration status against."
    steps:
      - run:
          name: Revert migrations
          when: on_fail
          command: |
            SUCCESS=$(curl --insecure https://kvdb.io/${KVDB_BUCKET}/migration_ << parameters.Workflow_ID >>)
            # Logic for reverting the database state
            if [[ $SUCCESS -eq 1 ]]; then
              cd ~/project/backend
              npm install
              npm run migration:revert
            fi

            
jobs:
  build-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Build front-end
          command: |
             cd frontend
              npm install
              npm run build

      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-deps
      # - notify_on_failure
  build-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end build
          command: |
            cd backend
            npm install
            npm run build
      - save_cache:
          paths: [backend/node_modules]
          key: backend-deps
      # - notify_on_failure

  test-frontend:
    docker:

      - image: cimg/node:13.8.0
    steps:
      - checkout 
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Front-end Unit Test
          command: |
            cd frontend
            npm install
            npm test
      # - notify_on_failure
   
                
  test-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end  Unit Test
          command: |
            cd backend
            npm install
            npm test
      # - notify_on_failure


            
  scan-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Front-end  scan
          command: |
            cd frontend
            npm install
            npm audit --audit-level=critical || true
      # - notify_on_failure
  scan-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end security scan
          command: |
            cd backend
            npm install
            npm audit fix --force --audit-level=critical
            npm audit fix --force --audit-level=critical
            npm audit --audit-level=critical
     
  deploy-infrastructure:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - run:
          name: Install AWS CLI
          command: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install
      - run:
          name: Ensure back-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/backend.yml \
              --tags project=udapeople\
              --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
              
      - run:
          name: Ensure front-end infrastructure exist
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=your-tag \
              --stack-name "udapeople-frontend-$(echo "${CIRCLE_WORKFLOW_ID:0:7}" | tr '[:upper:]' '[:lower:]')" \
              --parameter-overrides ID=$(echo "${CIRCLE_WORKFLOW_ID:0:7}" | tr '[:upper:]' '[:lower:]')  
              
      # - run:
      #     name: Add back-end ip to ansible inventory
      #     command: |  
      #       BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
      #         --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
      #         --query 'Reservations[*].Instances[*].PublicIpAddress' \
      #         --output text)
      #       echo $BACKEND_PUBLIC_IP >> .circleci/ansible/inventory.txt
      #       cat .circleci/ansible/inventory.txt
      # - persist_to_workspace:
      #     root: ~/
      #     paths:
      #       - project/.circleci/ansible/inventory.txt
      #  # - destroy-environment
      - run:
         name: Add back-end ip to ansible inventory
         command: |

                    echo "Attempting to fetch IP for tag: Name=backend-${CIRCLE_WORKFLOW_ID:0:7}"
       
                    BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
                      --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
                      --query 'Reservations[*].Instances[*].PublicIpAddress' \
                      --output text)

                    echo "IP Address Fetched: [$BACKEND_PUBLIC_IP]" # Debug echo

                    # Create/Overwrite the inventory file with the group header
                    # Make sure the path here is correct relative to the project root
                    echo "[web]" > .circleci/ansible/inventory.txt

                    # Append the fetched IP address to the inventory file
                    echo "$BACKEND_PUBLIC_IP" >> .circleci/ansible/inventory.txt

                    echo "Final contents of inventory file in this step:"
                    # Make sure the path here is correct relative to the project root
                    cat .circleci/ansible/inventory.txt        
                              
                    
            
 

      - persist_to_workspace:
          root: . # This root is fine
          paths:
            - .circleci/ansible/inventory.txt # This path seems correct assuming your working_directory is ~/project
      
###################
  configure-infrastructure:
    docker:
      - image: cimg/base:stable # Or similar base image
    steps:
      - checkout
      - install_ansible # This seems to be a custom or orb step
      - add_ssh_keys:
          fingerprints: ["SHA256:FJogcncZoAfN0hgvqhqDgAiiOafEii9G+HsAStU+Ok4"] # Placeholder for your actual fingerprint
      - attach_workspace:
          at: ~/
      - run:
          name: Configure Server
          command: |
            cd .circleci/ansible
            cat inventory.txt # This line was added to show the inventory content
            ansible-playbook -i inventory.txt configure-server.yml
  run-migrations:
    docker:
      - image:  cimg/node:13.8.0
    steps:
      - checkout
      - install_awscli
      - restore_cache:
          keys: [backend-build]
      - run:
          name: prepare environment for backend build
          command: |
            echo "ENVIRONMENT=production" > backend/.env
            echo "VERSION=1" >> backend/.env
            echo "TYPEORM_CONNECTION=postgres" >> backend/.env
            echo "TYPEORM_MIGRATIONS_DIR=./src/migrations" >> backend/.env
            echo "TYPEORM_ENTITIES=./src/modules/domain/**/*.entity.ts" >> backend/.env
            echo "TYPEORM_MIGRATIONS=./src/migrations/*.ts" >> backend/.env
            echo "TYPEORM_HOST=${TYPEORM_HOST}" >> backend/.env
            echo "TYPEORM_PORT=${TYPEORM_PORT}" >> backend/.env
            echo "TYPEORM_USERNAME=${TYPEORM_USERNAME}" >> backend/.env
            echo "TYPEORM_PASSWORD=${TYPEORM_PASSWORD}" >> backend/.env
            echo "TYPEORM_DATABASE=${TYPEORM_DATABASE}" >> backend/.env
            echo "DB_SSL=${DB_SSL}" >> backend/.env
            echo "NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}" >> backend/.env
            echo "PGSSLMODE=require" >> backend/.env
      - run:
          name: Install dependencies
          command: |
            export DEBIAN_FRONTEND=noninteractive
          
            sudo apt update
            sudo apt install -y awscli curl
            
      - run:
          name: Run migrations
          command: |
            cd backend
            npm install
            if [ -f .env ]; then
              export $(grep -v '^#' .env | xargs)
            fi
            if [ -z "${KVDB_BUCKET}" ]; then
              echo "ERROR: KVDB_BUCKET environment variable is not set. Cannot update KVDB."
              exit 1
            fi
            npm run migrations > ./migrations_dump.txt 2>&1
            migration_status_code=$?
            cat ./migrations_dump.txt
            if [[ "${migration_status_code}" -ne 0 ]]; then
              echo "ERROR: 'npm run migrations' command failed with exit code ${migration_status_code}."
              exit "${migration_status_code}"
            fi
            echo "Migration command completed successfully (exit code 0). Checking output for success messages..."
            if grep -E -q "has been executed successfully.|No migrations are pending" ./migrations_dump.txt; then
              echo "Found expected success message ('migration executed' or 'no migrations pending') in log."
              echo "--- Updating KVDB status key ---"
              curl -sf -X PUT \
                "https://kvdb.io/${KVDB_BUCKET}/migration_${CIRCLE_WORKFLOW_ID:0:7}" \
                -d '1' || {
                  curl_exit_code=$?
                  echo "ERROR: curl command failed to update kvdb.io with exit code ${curl_exit_code}."
                  echo "Check network connectivity, KVDB_BUCKET value ('${KVDB_BUCKET}'), and kvdb.io service status."
                  exit $curl_exit_code
                }
              echo "KVDB status updated successfully via curl."
            else
              echo "WARNING: Migration command succeeded (exit code 0), but NEITHER 'has been executed successfully.' NOR 'No migrations are pending' was found in the output."
              echo "Skipping KVDB update due to unrecognized success output."
            fi
            echo "Migration and KVDB update check process finished successfully."
      - destroy-environment
      - revert-migrations


             
        

  deploy-frontend:
    docker: 
    - image: cimg/base:stable
    steps:
      - checkout
      - install_awscli
      - install_nodejs
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Install dependencies
          command: |
            cd frontend
            npm install
      - run:
          name: Get backend url
          command: |
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --query 'Reservations[*].Instances[*].PublicIpAddress' \
              --output text)

            echo "API_URL=http://${BACKEND_PUBLIC_IP}:3030" >> frontend/.env
            cat frontend/.env
      - run:
          name: Deploy frontend objects
          command: |
            cd frontend
            npm run build
            aws s3 cp dist s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7} --recursive

  
  deploy-backend:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - install_awscli
      - install_nodejs
      - install_ansible
      - add_ssh_keys:
          fingerprints: [" SHA256:FJogcncZoAfN0hgvqhqDgAiiOafEii9G+HsAStU+Ok4"]
      - attach_workspace:
          at: ~/
      - run:
          name: Install dependencies
          command: |
            sudo apt -y install tar gzip curl
      - run:
          name: Deploy backend
          command: |
            cd backend
            npm install
            npm run build
            
            cd ..
            tar -C backend -czvf artifact.tar.gz .
            cp artifact.tar.gz .circleci/ansible/roles/deploy/files
            
            export TYPEORM_MIGRATIONS_DIR=./migrations
            export TYPEORM_ENTITIES=./modules/domain/**/*.entity{.ts,.js}
            export TYPEORM_MIGRATIONS=./migrations/*.ts
            
            cd .circleci/ansible
            echo "Contents  of the inventory.txt file is -------"
            cat inventory.txt
            ansible-playbook -i inventory.txt deploy-backend.yml

      - run:
            name: Package Backend
            command: |
              # --- Configure shell options ---
              #!/bin/bash -eo pipefail
              # Use '-e' to exit immediately if a command exits with a non-zero status.
              # Use '-o pipefail' to fail if any command in a pipeline fails.

              echo "--- Starting Package Backend Step ---"
              echo "Current directory (project root): $(pwd)"
              # We assume CircleCI checked out the project to this location.

              # --- Change directory to backend ---
              echo "Changing to backend/ directory..."
              # Use 'if ! command; then error && exit; fi' structure for robust error handling
              if ! cd backend; then
                echo "ERROR: Failed to change directory to ./backend!" >&2 # Output error to stderr
                exit 1
              fi
              echo "Current directory is now: $(pwd)"
              echo "Listing contents of backend/ directory:"
              ls -la # Show files/folders inside the backend directory

              # --- Install Node Dependencies ---
              echo "Ensuring dependencies are installed (running npm install --production=false)..."
              # '--force-consistent-casing=false' is a common workaround for case-sensitivity issues on some file systems/zip tools, safer for CI.
              # '--production=false' ensures devDependencies are installed, which are often needed for the build process (like TypeScript).
              if ! npm install --force-consistent-casing=false --production=false; then
                echo "ERROR: 'npm install --production=false' failed!" >&2
                exit 1
              fi

              # --- Run Backend Build ---
              echo "Running npm run build (which should execute tsc)..."
              # Make sure your backend's package.json has a 'build' script that runs your compiler (e.g., tsc).
              if ! npm run build; then
                echo "ERROR: 'npm run build' failed!" >&2
                exit 1
              fi

              echo "Build finished. Checking for built output directory (dist/)..."
              # --- Check for the output directory ---
              if [ -d "dist" ]; then
                echo "SUCCESS: 'dist/' directory was created by the build."
                echo "Contents of dist/ directory:"
                ls -la dist

                # *** Crucial Check: Does the expected entry file exist in the build output? ***
                # BASED ON PREVIOUS LOGS, your build outputs dist/main.js
                # PM2 WILL LOOK FOR THE SCRIPT SPECIFIED IN ecosystem.config.js (hopefully main.js now)
                BUILD_ENTRY_FILE="dist/main.js" # <--- SET THIS TO THE FILE YOUR BUILD ACTUALLY CREATES

                if [ -f "$BUILD_ENTRY_FILE" ]; then
                  echo "SUCCESS: Expected build entry file '$BUILD_ENTRY_FILE' exists."
                else
                  echo "ERROR: Expected build entry file '$BUILD_ENTRY_FILE' DOES NOT EXIST inside 'dist/' directory." >&2
                  echo "Check your backend's 'package.json' build script, 'tsconfig.json outDir', and entry file name." >&2
                  exit 1
                fi
              else
                echo "CRITICAL ERROR: 'dist/' directory was NOT created by 'npm run build'!" >&2
                echo "This means your backend compilation/build step failed or is misconfigured." >&2
                echo "Check your backend's 'package.json' build script and 'tsconfig.json'." >&2
                exit 1
              fi

              # --- Check for ecosystem.config.js before packaging ---
              BACKEND_ARTIFACT_NAME="artifact.tar.gz" # Consistent name for the tarball
              # List the files that should be included in the artifact tarball
              TAR_INCLUDES="dist/ package.json package-lock.json" # Always include build output and package files
              if [ -f "ecosystem.config.js" ]; then
                echo "ecosystem.config.js found. Including it in the artifact."
                TAR_INCLUDES="$TAR_INCLUDES ecosystem.config.js" # Add if found
              else
                echo "WARNING: ecosystem.config.js not found in backend/ root. (Ansible deploy step might fail later)." >&2
                # We'll still tar without it, but Ansible might complain if it's needed for PM2
                # The check in the deploy step for ecosystem.config.js presence on server might need to be adjusted or skipped if it's truly optional
              fi

              echo "Running tar command to create '$BACKEND_ARTIFACT_NAME' in ./backend/ ..."
              echo "Tar command includes: $TAR_INCLUDES"
              # Running tar from the backend/ directory
              # Using '--warning=no-file-ignored' to avoid verbose warnings if node_modules are excluded by .gitignore etc.
              if ! tar --warning=no-file-ignored -czf "$BACKEND_ARTIFACT_NAME" $TAR_INCLUDES; then
                echo "ERROR: 'tar' command failed!" >&2
                exit 1 # Exit if tar failed
              fi
              echo "Successfully created '$BACKEND_ARTIFACT_NAME' in ./backend/."

              # --- Verify the created artifact file exists and check its contents ---
              # Log cuts off around here, let's verify the file exists
              if [ ! -f "$BACKEND_ARTIFACT_NAME" ]; then
                  echo "CRITICAL ERROR: Artifact file '$BACKEND_ARTIFACT_NAME' was not created by tar!" >&2
                  exit 1
              fi
              echo "Artifact file '$BACKEND_ARTIFACT_NAME' verified to exist in ./backend."
              ls -lh "$BACKEND_ARTIFACT_NAME" # Show its size clearly

              echo "Verifying contents and structure of '$BACKEND_ARTIFACT_NAME'..."
              # 'tar -tzf' lists contents. Piping to /dev/null is okay for just checking exit status.
              if ! tar -tzf "$BACKEND_ARTIFACT_NAME" > /dev/null; then
                  echo "ERROR: Verifying contents of artifact file failed ('tar -tzf' returned non-zero)!" >&2
                  exit 1 # Exit if tar -tzf failed
              fi
              echo "Artifact contents verification successful." # Log should now get here if tar -tzf passed


              # --- Change directory back to project root ---
              echo "Changing back to project root directory..."
              if ! cd ..; then
                echo "ERROR: Failed to change directory back to project root (from backend/)!" >&2
                exit 1
              fi
              echo "Current directory is now: $(pwd)" # Should be /home/circleci/project

              # --- Create Ansible artifact target directory (if needed) ---
              # This is where the deploy playbook expects the artifact to be found.
              # Path is relative to the project root.
              ANSIBLE_FILES_DIR=".circleci/ansible/roles/deploy/files"
              echo "Ensuring Ansible artifact target directory '$ANSIBLE_FILES_DIR' exists..."
              # mkdir -p: Creates parent directories if needed, doesn't error if dir exists.
              if ! mkdir -p "$ANSIBLE_FILES_DIR"; then
                echo "ERROR: Failed to create Ansible files directory '$ANSIBLE_FILES_DIR'!" >&2
                exit 1
              fi
              # Verify the directory exists (optional but adds robustness)
              if [ ! -d "$ANSIBLE_FILES_DIR" ]; then
                  echo "CRITICAL ERROR: Ansible files directory '$ANSIBLE_FILES_DIR' does not exist after mkdir -p!" >&2
                  exit 1
              fi
              echo "Ansible target directory '$ANSIBLE_FILES_DIR' verified to exist."

              # --- Copy the created artifact to the Ansible target directory ---
              # Source path is relative to current dir (project root), file is in ./backend
              # Destination path is relative to current dir (project root)
              echo "Copying artifact from ./backend/'$BACKEND_ARTIFACT_NAME' to './$ANSIBLE_FILES_DIR/'"
              if ! cp backend/"$BACKEND_ARTIFACT_NAME" "$ANSIBLE_FILES_DIR"/; then
                echo "ERROR: Copying artifact file '$BACKEND_ARTIFACT_NAME' to '$ANSIBLE_FILES_DIR/' failed!" >&2
                exit 1 # Exit if copy failed
              fi
              echo "Artifact file copied successfully."

              # --- Final verification in the target directory ---
              echo "Verifying artifact is in the final target directory:"
              if [ -f "$ANSIBLE_FILES_DIR/$BACKEND_ARTIFACT_NAME" ]; then
                echo "SUCCESS: Final artifact file '$ANSIBLE_FILES_DIR/$BACKEND_ARTIFACT_NAME' found and verified."
                ls -lh "$ANSIBLE_FILES_DIR/$BACKEND_ARTIFACT_NAME" # Show final file
              else
                echo "CRITICAL ERROR: Final artifact file '$ANSIBLE_FILES_DIR/$BACKEND_ARTIFACT_NAME' NOT FOUND after copy!" >&2
                exit 1
              fi

              echo "--- Finished Package Backend Step ---"
              # The script should naturally exit with status 0 if it reaches here

      # - destroy-environment
      # - revert-migrations
  smoke-test:
    docker:
    - image: cimg/base:stable
    steps:
    - checkout
    - run:
        name: Install dependencies
        command: |
          sudo apt update
          sudo apt upgrade -y
          sudo apt install -y curl awscli
          aws --version
    - run:
        name: Backend smoke test
        command: |
          #!/bin/bash -eo pipefail

          STACK_SUFFIX=${CIRCLE_WORKFLOW_ID:0:7}
          BACKEND_PUBLIC_IP=$(aws ec2 describe-instances --filters "Name=tag:aws:cloudformation:stack-name,Values=udapeople-backend-${STACK_SUFFIX}" "Name=instance-state-name,Values=running" --query 'Reservations[].Instances[].PublicIpAddress' --output text)

          if [ -z "$BACKEND_PUBLIC_IP" ]; then
            echo "Error: No running EC2 instances found with the specified stack tag: udapeople-backend-${STACK_SUFFIX}."
            exit 1
          fi

          echo "Retrieved BACKEND_PUBLIC_IP: $BACKEND_PUBLIC_IP"

          export API_URL=http://${BACKEND_PUBLIC_IP}:3030

          echo "Starting backend API smoke test with URL: ${API_URL}/api/status"

          MAX_RETRIES=20 # Increased retries, backend might take longer
          SLEEP_SECONDS=10 # Increased sleep time between retries
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "Attempt $RETRY_COUNT of $MAX_RETRIES: Pinging ${API_URL}/api/status"

            # Attempt the curl request
            # -s: Silent mode
            # --connect-timeout 10: Increased connect timeout
            # --max-time 20: Increased max time for the whole operation
            # 2>&1: Redirect stderr to stdout to capture error messages in the variable
            CURL_OUTPUT=$(curl -s --connect-timeout 10 --max-time 20 ${API_URL}/api/status 2>&1)
            CURL_STATUS=$? # Capture exit status of curl immediately

            # Explicitly log the curl result
            echo "  Curl command finished with exit status: $CURL_STATUS"
            echo "  Curl output (response body or error message): '$CURL_OUTPUT'"

            if [ $CURL_STATUS -eq 0 ]; then
              # Curl succeeded, now check the response body
              echo "  Curl request was successful (status 0)."
              if echo "$CURL_OUTPUT" | grep -q "ok"; then
                echo "Smoke test passed: Found 'ok' in the API response body."
                exit 0 # Success! Exit the script
              else
                # Curl succeeded, but the response didn't have "ok"
                echo "  Response did not contain 'ok'. Backend might be up but not fully ready or endpoint is wrong."
              fi
            else
              # Curl failed
              echo "  Curl request failed with status $CURL_STATUS. Cannot reach the API endpoint."
              # Common failures:
              # 6: Couldn't resolve host
              # 7: Failed to connect() - often network/firewall issues or service not listening
              # 28: Operation timed out
              echo "  Possible causes: Backend service not running, firewall blocking access, incorrect IP/port."
            fi

            # If not successful and not the last retry, wait before retrying
            if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
              echo "  Waiting ${SLEEP_SECONDS}s before next attempt..."
              sleep $SLEEP_SECONDS
            fi
          done # End of while loop

          # If the loop finishes without exiting, it means all retries failed
          echo "----------------------------------------------------"
          echo "SMOKE TEST FAILED AFTER $MAX_RETRIES ATTEMPTS."
          echo "The backend API did not become reachable at ${API_URL}/api/status or did not respond with 'ok'."
          echo "Review the deployment logs for potential backend startup errors and ensure network/firewall rules allow access on port 3030 from CircleCI."
          echo "Last attempt status: $CURL_STATUS"
          echo "Last attempt output: '$CURL_OUTPUT'"
          exit 1 # Indicate failure

    - run:
        name: Frontend smoke test.
        command: |
            URL="http://udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website.${AWS_DEFAULT_REGION}.amazonaws.com/#/employees"
            echo ${URL}
            if curl -s ${URL} | grep "Welcome"
            then
            echo SUCCESS
            exit 0
            else
            echo FAIL
            exit 1
            fi
 

workflows:
  default:
    jobs:
      - build-frontend
      - build-backend
      - test-frontend:
          requires: [build-frontend]
      - test-backend:
          requires: [build-backend]
      - scan-backend:
          requires: [build-backend]
      - scan-frontend:
          requires: [build-frontend]
 
      - deploy-infrastructure:
          requires: [test-frontend, test-backend, scan-frontend, scan-backend]
          filters:
            branches:
              only: [master]
      - configure-infrastructure:
          requires: [deploy-infrastructure]
      - run-migrations:
          requires: [configure-infrastructure]
      - deploy-frontend:
          requires: [run-migrations]
      - deploy-backend:
          requires: [run-migrations]
      - smoke-test:
          requires: [deploy-backend, deploy-frontend]
 

